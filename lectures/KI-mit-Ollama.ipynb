{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4179492-9ce4-4cb9-bb37-8b5b5ff1646d",
   "metadata": {},
   "source": [
    "# KI in deinen Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec67ab2-68a3-4473-9015-311ec4c11016",
   "metadata": {},
   "source": [
    "https://github.com/ollama/ollama-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec3b3e2-7ebb-48ab-8c18-fe984dc839d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "# default: http://127.0.0.1:11434 (Ollama läuft in der JupyterHub-VM)\n",
    "# optional: http://10.0.2.2:11434 (Ollama läuft auf dem Host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb804497-b23d-4336-8817-339fb6bf2022",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl http://127.0.0.1:11434"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d7abec-f427-415f-bf23-e9fbcf843c70",
   "metadata": {},
   "source": [
    "So baust du KI in deine Notebooks ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cd32b0-adb2-4436-b395-27bdd13d5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message senden\n",
    "response = chat(\n",
    "    model='gemma3:270m', \n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'Why is the sky usually blue?'},\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Antwort komplett ausgeben\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1db9d2-1e98-4cd9-9044-3ddd2c89a49f",
   "metadata": {},
   "source": [
    "Wenn du die Ausgabe Zeile für Zeile möchtest, geht das so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c1964-6b8f-4baa-9fd0-a797c535cc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noch eine Nachricht an den Ollama-Server senden\n",
    "stream = chat(\n",
    "    model='gemma3:270m',\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'Why is the sky sometimes red?'}\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "# Diesmal die Antwort Zeile für Zeile ausgeben:\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d368c7-56c8-45be-b44a-9e51a80d736f",
   "metadata": {},
   "source": [
    "## Ollama in der JupyterHub-VM\n",
    "\n",
    "Für kleine Modelle genügt die in der VM eingebaute Ollama-Instanz. Die verfügbaren Modelle lässt du dir so anzeigen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0003ce3-b157-445a-b63a-cb933b20df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8638a0-7389-4db4-9dbd-21811fa9487b",
   "metadata": {},
   "source": [
    "Informationen zu einem einzelenen MOdell, bspw. seine Grösse, erhälts du so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32d951b-6b21-4b4e-9810-e86897187313",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ollama show gemma3:270m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8d343d-a38b-4acb-bf57-4bc0bd53ed8a",
   "metadata": {},
   "source": [
    "Welche Modelle gerade ausgeführt werden, zeigt dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d7bf38-bf8b-4e19-8edd-d1a12f079dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ollama ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989b2944-028d-4b7a-b6a7-bccaaf770ca5",
   "metadata": {},
   "source": [
    "Neue Modelle lädst du, indem du unter https://ollama.com/search dir eines aussuchst - bspw. `qwen3:0.6b` - und dann mit diesem Befehl herunterlädst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4077ee7e-7901-4a81-9d12-5bebb5e70ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ollama pull qwen3:0.6b &> output.txt\n",
    "cat output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8c3fdc-b1b3-4077-ad4f-c5c45e0931d4",
   "metadata": {},
   "source": [
    "Achtung, schau dir **VORHER** auf https://ollama.com/search genau an, wie gross ein Modell ist. Modelle über 1 GB werden in der VM kaum performant laufen. Wenn du ein grössees Modell laufen lassen willst, nutzte dazu ein Ollama auf deinem Host-Rechner. (Siehe weiter unten)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c701802-a394-4e64-9edb-231b37f780e8",
   "metadata": {},
   "source": [
    "Du löschst ein Modell mit diesem Befehl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e6105-1b22-4a17-a382-0fa0f9ef3c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ollama rm NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b51cd0-0527-47db-ae80-43cc858094ee",
   "metadata": {},
   "source": [
    "Für NAME setze einen der Namen ein, den dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842fc487-3106-494d-b639-7402de695b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a328387d-c480-4dde-9bda-2d72d7c4bc7a",
   "metadata": {},
   "source": [
    "angezeigt hat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe03479f-4d4a-48e9-a24a-798a0c721a01",
   "metadata": {},
   "source": [
    "## Ollama-Server vom Host nutzen\n",
    "\n",
    "Du willst grössere Modell nutzen, die nicht in der VM laufen? Kein Problem!\n",
    "\n",
    "**Wenn**\n",
    "\n",
    "- [Ollama](https://ollama.com/download) auf dem Host läuft (mehr verfügbarer RAM und CPU-Kerne sowie ggf. Hardwarebeschleunigung durch deine GPU)\n",
    "- **Und** du unter Settings \"Expose Ollama to the Network\" aktiviert hast\n",
    "- **Und** unter VirtualBox oder UTM eine passende Port-Weiterleitungs-Regel existiert\n",
    "\n",
    "**Dann**\n",
    "- kannst du aus der VM unter der URL http://10.0.2.2:11434 auf Ollama auf dem Host zugreifen.\n",
    "\n",
    "Überprüfe ggf. die Weiterleitungsregeln in der VM unter: Ändern -> Netzwerk -> Port Weiterleitung\n",
    "\n",
    "|Name|Protokoll|Host-IP|Host-Port|Gast-IP|Gast-Port|\n",
    "|--|--|--|--|--|--|\n",
    "|ollama-ext|TCP|10.0.2.2|11434|127.0.0.1|11434|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1a991b-f0e3-4f93-9f36-74d071fc3633",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl http://10.0.2.2:11434"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e89ebad-ed29-495d-a657-512a897202e6",
   "metadata": {},
   "source": [
    "So baust du KI, die nicht in der VM selbst läuft, in deine Notebooks ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02396c3e-fbe7-4982-9e34-d69dc6435036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9e170e-becc-416e-b6c2-97f7f3e5e1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(\n",
    "  host='http://10.0.2.2:11434'\n",
    ")\n",
    "response = client.chat(model='gemma3:270m', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'What is the best Programming Language??',\n",
    "  },\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
