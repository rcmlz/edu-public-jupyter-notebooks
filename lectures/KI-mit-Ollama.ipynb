{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4179492-9ce4-4cb9-bb37-8b5b5ff1646d",
   "metadata": {},
   "source": [
    "# KI in deinen Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec67ab2-68a3-4473-9015-311ec4c11016",
   "metadata": {},
   "source": [
    "https://github.com/ollama/ollama-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec3b3e2-7ebb-48ab-8c18-fe984dc839d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "# default: http://127.0.0.1:11434 (Ollama läuft in der JupyterHub-VM)\n",
    "# optional: http://10.0.2.2:11434 (Ollama läuft auf dem Host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb804497-b23d-4336-8817-339fb6bf2022",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl http://127.0.0.1:11434"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d7abec-f427-415f-bf23-e9fbcf843c70",
   "metadata": {},
   "source": [
    "So baust du KI in deine Notebooks ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cd32b0-adb2-4436-b395-27bdd13d5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message senden\n",
    "response = chat(\n",
    "    model='gemma3:270m', \n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'Why is the sky usually blue?'},\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Antwort komplett ausgeben\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1db9d2-1e98-4cd9-9044-3ddd2c89a49f",
   "metadata": {},
   "source": [
    "Wenn du die Ausgabe Zeile für Zeile möchtest, geht das so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c1964-6b8f-4baa-9fd0-a797c535cc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noch eine Nachricht an den Ollama-Server senden\n",
    "stream = chat(\n",
    "    model='gemma3:270m',\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'Why is the sky sometimes red?'}\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "# Diesmal die Antwort Zeile für Zeile ausgeben:\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d368c7-56c8-45be-b44a-9e51a80d736f",
   "metadata": {},
   "source": [
    "## Ollama in der JupyterHub-VM\n",
    "\n",
    "Für kleine Modelle genügt die in der VM eingebaute Ollama-Instanz. Die verfügbaren Modelle lässt du dir so anzeigen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0003ce3-b157-445a-b63a-cb933b20df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8638a0-7389-4db4-9dbd-21811fa9487b",
   "metadata": {},
   "source": [
    "Informationen zu einem einzelenen MOdell, bspw. seine Grösse, erhälts du so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32d951b-6b21-4b4e-9810-e86897187313",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ollama show gemma3:270m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8d343d-a38b-4acb-bf57-4bc0bd53ed8a",
   "metadata": {},
   "source": [
    "Welche Modelle gerade ausgeführt werden, zeigt dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d7bf38-bf8b-4e19-8edd-d1a12f079dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ollama ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989b2944-028d-4b7a-b6a7-bccaaf770ca5",
   "metadata": {},
   "source": [
    "Neue Modelle lädst du, indem du unter https://ollama.com/search dir eines aussuchst - bspw. `qwen3:0.6b` - und dann mit diesem Befehl herunterlädst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4077ee7e-7901-4a81-9d12-5bebb5e70ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ollama pull qwen3:0.6b &> output.txt\n",
    "cat output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8c3fdc-b1b3-4077-ad4f-c5c45e0931d4",
   "metadata": {},
   "source": [
    "Achtung, schau dir **VORHER** auf https://ollama.com/search genau an, wie gross ein Modell ist. Modelle über 1 GB werden in der VM kaum performant laufen. Wenn du ein grösseres Modell laufen lassen willst, nutzte dazu ein Ollama auf deinem Host-Rechner. (Siehe weiter unten)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c701802-a394-4e64-9edb-231b37f780e8",
   "metadata": {},
   "source": [
    "Du löschst ein Modell mit diesem Befehl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e6105-1b22-4a17-a382-0fa0f9ef3c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ollama rm NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b51cd0-0527-47db-ae80-43cc858094ee",
   "metadata": {},
   "source": [
    "Für NAME setze einen der Namen ein, den dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842fc487-3106-494d-b639-7402de695b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a328387d-c480-4dde-9bda-2d72d7c4bc7a",
   "metadata": {},
   "source": [
    "angezeigt hat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe03479f-4d4a-48e9-a24a-798a0c721a01",
   "metadata": {},
   "source": [
    "## Ollama-Server vom Host nutzen\n",
    "\n",
    "Du willst grössere Modell nutzen, die nicht in der VM laufen? Kein Problem!\n",
    "\n",
    "**Wenn**\n",
    "\n",
    "- [Ollama](https://ollama.com/download) auf dem Host läuft.\n",
    "  - Vorteile: mehr RAM, CPU-Kerne und SSD-Speicher sowie ggf. Hardwarebeschleunigung durch deine GPU.\n",
    "- **Und** du unter Settings \"Expose Ollama to the Network\" aktiviert hast.\n",
    "- **Und** unter VirtualBox oder UTM eine passende Port-Weiterleitungs-Regel existiert.\n",
    "\n",
    "**Dann**\n",
    "- kannst du aus Jupyter unter der URL `http://10.0.2.2:11434` auf Ollama auf dem Host zugreifen.\n",
    "\n",
    "Überprüfe ggf. die Weiterleitungsregeln in der VM unter: Ändern -> Netzwerk -> Port Weiterleitung\n",
    "\n",
    "|Name|Protokoll|Host-IP|Host-Port|Gast-IP|Gast-Port|\n",
    "|--|--|--|--|--|--|\n",
    "|ollama-ext|TCP|10.0.2.2|11434|127.0.0.1|11434|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aaa893-09b3-4230-abdf-f33f30c169d9",
   "metadata": {},
   "source": [
    "So testest du, ob der der Ollama-Server erreichbar ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1a991b-f0e3-4f93-9f36-74d071fc3633",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl http://10.0.2.2:11434"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e89ebad-ed29-495d-a657-512a897202e6",
   "metadata": {},
   "source": [
    "So nutzt du ein LLM, das nicht in der VM selbst läuft, sondern auf dem Host:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02396c3e-fbe7-4982-9e34-d69dc6435036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5309aa72-b809-47d4-b900-8cf5cb4cf686",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(\n",
    "  host='http://10.0.2.2:11434'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92768fd1-3f53-4f8b-a8d5-f0fcf707b90f",
   "metadata": {},
   "source": [
    "Welche Modelle gibt es auf dem externen Ollama?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89039034-71ef-4856-ba03-d6bf6a67dc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_models(client):\n",
    "    \"\"\"Zeigt die verfügbares Modelle an\"\"\"\n",
    "    for entry in client.list()['models']:\n",
    "      print(entry[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd746e37-5876-4d2b-b4cb-856330b8bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_models(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e59139-df76-4b94-b559-db428c2d11bc",
   "metadata": {},
   "source": [
    "Jetzt kannst du wie gewohnt die Abfragen ausführen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9e170e-becc-416e-b6c2-97f7f3e5e1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat(model='gemma3:270m', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'What is the best Programming Language??',\n",
    "  },\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129974f3-cd68-4671-8f6b-7f7cae4567e6",
   "metadata": {},
   "source": [
    "und die Antwort komplett ausgeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a8fd31-8312-4035-ac7c-aecb883f3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b7c17-f77f-4c56-9c90-289edda06ca8",
   "metadata": {},
   "source": [
    "Neue Modelle installierst entweder direkt auf dem Host oder von Jupyter aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6542e525-f913-4415-8dc7-86243229f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.pull('qwen3:0.6b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b047232-f02f-4077-9f4c-fb665b6d0c15",
   "metadata": {},
   "source": [
    "Teste auch diesen Befehl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0656010a-1eeb-4d07-8bb9-429bbb8c5dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.ps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0478e030-45c5-4422-8c9d-32586b9856ac",
   "metadata": {},
   "source": [
    "und ggf. auch das Löschen eines (kleinen) Modells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320fe4e8-7bec-4524-b5d1-64e93a4f62ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_models(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5db4a9-292c-40b5-a43e-a7e828a89ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete('NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d54f44-9d46-470a-a412-5702e4a03c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_models(client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
